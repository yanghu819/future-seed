0.557452 M parameters
step 0: train loss 2.8332, val loss 2.8332, time 0.49 seconds
shift acc 0.0978
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
A=34151452968666020011000212112121212021211001001222002200222211212200200001010120200121001202111020220112112210022002122000200020221010220211010011221200212001200021220221101212121022022212221100110011201021122122000220100210211220012101110211221211122210

step 50: train loss 2.3114, val loss 2.3117, time 31.15 seconds
shift acc 0.0988
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
A=09729150103877998898992899982888988288292898999222298989898292992992989299989989982998288299928929928292999289298898929228289898928999828998829992298899999299998988828988988992299889298829829998992228899999822899288282992298898989982928999299892928228999

step 99: train loss 2.3083, val loss 2.3066, time 61.25 seconds
shift acc 0.0950
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
A=86325625432957843334383448484488833484833344338848833383344334483344388383834843848333333334883883388834433843444433433388433388433448838333343384484438438343383888388383383338444848444838443388344338484848888388433344448443438388388844884343833844434884

Total training time: 71.40 seconds
Total steps: 1 for 1 tokens
Avg decoded per step: 1.00

Output:
A=072856470891433
