0.557452 M parameters
step 0: train loss 2.8332, val loss 2.8332, time 0.38 seconds
constr acc 0.1231
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
P=39689277|M=791020011000212112121212021211001001222002200222211100222010100112020001020112012012020200021211022110211220202121202012012220121021200002002000201220010222120000012220012120211200111100120110210000211012212111012110102112102011100121200202201

step 50: train loss 2.2607, val loss 2.2662, time 21.63 seconds
constr acc 0.1669
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
P=61087963|M=378600707007067707600767700067777777770666700660660666676000700066776006006670767700666060667777767666777007077607600670760766767076006706770677076076606660077006006066776600677067007777070000760066666600066066777677006067000067006000077607770

step 99: train loss 2.1264, val loss 2.1344, time 43.03 seconds
constr acc 0.2137
Total steps: 240 for 240 tokens
Avg decoded per step: 1.00
Sample:
P=76611917|M=777771179899799988797779779798997878977879878779997877779889798787198997777797977798787877577155317351555111519887272444887244487271735757155571515555335117353355355351199719888989779977719711171117279888744487287377517555111777997197989888898

Total training time: 50.60 seconds
Total steps: 1 for 1 tokens
Avg decoded per step: 1.00

Output:
P=96944054|M=4252
